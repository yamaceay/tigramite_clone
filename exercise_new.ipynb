{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal discovery and effect estimation case study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.1-cp311-cp311-macosx_11_0_arm64.whl (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.18 in /opt/homebrew/lib/python3.11/site-packages (from statsmodels) (1.24.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /opt/homebrew/lib/python3.11/site-packages (from statsmodels) (1.11.1)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from statsmodels) (2.1.1)\n",
      "Collecting patsy>=0.5.4\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.9/233.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21.3 in /Users/yamacay/Library/Python/3.11/lib/python/site-packages (from statsmodels) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yamacay/Library/Python/3.11/lib/python/site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in /Users/yamacay/Library/Python/3.11/lib/python/site-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "Successfully installed patsy-0.5.6 statsmodels-0.14.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline     \n",
    "\n",
    "# first install tigramite developer branch from https://github.com/jakobrunge/tigramite.git\n",
    "import tigramite\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite.toymodels import structural_causal_processes\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.pcmci import PCMCI\n",
    "#from tigramite.independence_tests import ParCorr, GPDC, CMIknn, RobustParCorr\n",
    "from tigramite.independence_tests.oracle_conditional_independence import OracleCI\n",
    "from tigramite.independence_tests.parcorr import ParCorr\n",
    "from tigramite.independence_tests.gpdc import GPDC\n",
    "from tigramite.independence_tests.cmiknn import CMIknn\n",
    "from tigramite.independence_tests.robust_parcorr import RobustParCorr\n",
    "\n",
    "from tigramite.causal_effects import CausalEffects\n",
    "from tigramite.lpcmci import LPCMCI\n",
    "\n",
    "import statsmodels\n",
    "# Seaborn for nice scatter plots\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset_no1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m which \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynthetic\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#'climate'\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynthetic\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_no1.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     var_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$X^0$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$X^1$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$X^2$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$X^3$\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset_no1.npy'"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Load dataset\n",
    "which = 'synthetic' #'climate'\n",
    "\n",
    "if which == 'synthetic':\n",
    "    data = np.load(\"dataset_no1.npy\")\n",
    "    var_names = [r'$X^0$', r'$X^1$', r'$X^2$', r'$X^3$']\n",
    "else:\n",
    "    data = np.loadtxt(\"WPAC_CPAC_EPAC_ATL.txt\")\n",
    "    var_names = [\"WPAC\", \"CPAC\", \"EPAC\", \"ATL\"]\n",
    "\n",
    "\n",
    "# data = model_data(1000)\n",
    "\n",
    "# Specify variable names\n",
    "\n",
    "# Initialize tigramite dataframe object\n",
    "dataframe = pp.DataFrame(data, var_names=var_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-analysis steps\n",
    "\n",
    "- Check stationarity\n",
    "- Check marginal and joint distributions\n",
    "- Check lagged dependencies\n",
    "\n",
    "\n",
    "First, we plot the time series. This can be done with the function ``tp.plot_timeseries``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.plot_timeseries(dataframe); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks stationary (how to test this?) and doesn't contain missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearity and noise distributions\n",
    "\n",
    "Here we use kernel dendity plots to get an impression of the type of dependencies (linear, nonlinear) and noise distributions (for example, Gaussian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "df = pd.DataFrame(data)\n",
    "sns.pairplot(df, diag_kind = 'kde'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable 1 seems to be non-Gaussian, the other's have more Gaussian marginals. From the density plots there is no obvious nonlinearity. There is much more one can test, but let's continue with causal methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help choose a causal discovery method for time series, we need to know the maximal time delay of direct causal relations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time delays\n",
    "\n",
    "We assess  whether time delays are present in the time series by looking at the lagged cross correlation function (because the joint density plots did not indicate nonlinear dependencies here, otherwise use nonlinear method). This can help to identify which maximal time lag ``tau_max`` to choose in the causal algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_test = ParCorr() #(significance='analytic')\n",
    "# ci_test = RobustParCorr() #(significance='analytic')\n",
    "\n",
    "pcmci = PCMCI(\n",
    "    dataframe=dataframe, \n",
    "    cond_ind_test=ci_test,\n",
    "    verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = pcmci.get_lagged_dependencies(tau_max=10, val_only=True)['val_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_func_matrix = tp.plot_lagfuncs(val_matrix=correlations, setup_args={'var_names':var_names, \n",
    "                                    'x_base':5, 'y_base':.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dependencies decay beyond a maximum lag of around 3, we can choose ``tau_max=3`` for causal inference methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal discovery\n",
    "\n",
    "- Granger causality\n",
    "- PCMCI causal discovery algorithm\n",
    "- PCMCI+ causal discovery algorithm\n",
    "- LiNGAM causal discovery method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def granger_causality(data, i, j, maxlags=5, alpha_lev=0.05):\n",
    "    \"\"\"Granger causality test using statsmodels.\"\"\"\n",
    "    \n",
    "    import statsmodels.tsa.api as tsa\n",
    "\n",
    "    tsamodel = tsa.var.var_model.VAR(data)\n",
    "    results = tsamodel.fit(maxlags=maxlags)\n",
    "\n",
    "    return results.test_causality(j, causing=i).pvalue < alpha_lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_lev = 0.01\n",
    "for i in range(data.shape[1]):\n",
    "    for j in range(i+1, data.shape[1]):\n",
    "        # Granger\n",
    "        granger_result = granger_causality(data[:1000], i=i, j=j, maxlags=5, alpha_lev=alpha_lev)\n",
    "        if granger_result == 1:\n",
    "            print(\"Granger causality %s --> %s\" % (var_names[i], var_names[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCMCI\n",
    "\n",
    "Recall that PCMCI assumes lagged dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "# parcorr = ParCorr(significance='analytic')\n",
    "# pcmci = PCMCI(\n",
    "#     dataframe=dataframe, \n",
    "#     cond_ind_test=parcorr,\n",
    "#     verbosity=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significance level, here more a hyper-parameter than a statistical parameter\n",
    "alpha_level = 0.01\n",
    "tau_max = 2\n",
    "results = pcmci.run_pcmci(tau_max=tau_max, tau_min=0, alpha_level=alpha_level)\n",
    "tp.plot_time_series_graph(graph=results['graph'], save_name=None, \n",
    "            var_names=var_names, \n",
    "                          figsize=(6, 5), \n",
    "                        )\n",
    "tp.plot_graph(graph=results['graph'], save_name=None, \n",
    "            var_names=var_names, \n",
    "                          figsize=(6, 5), \n",
    "                        ); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCMCI+ algorithm\n",
    "\n",
    "Now we use the PCMCI+ algorithm that can also cope with contemporaneous links. We choose partial correlation, thus limiting ourselves to linear dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_lev = 0.01\n",
    "tau_max = 2\n",
    "results = pcmci.run_pcmciplus(pc_alpha=alpha_lev, \n",
    "                    tau_min=0, tau_max=tau_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.plot_time_series_graph(graph=results['graph'], save_name=None, \n",
    "            var_names=var_names, \n",
    "                          figsize=(6, 5), \n",
    "                        )\n",
    "tp.plot_graph(graph=results['graph'], save_name=None, \n",
    "            var_names=var_names, \n",
    "                          figsize=(6, 5), \n",
    "                        ); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCMCI+ finds links 1-->2-->3 and an undirected link 0--1 due to Markov equivalence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpcmci = LPCMCI(\n",
    "    dataframe=dataframe, \n",
    "    cond_ind_test=ci_test,\n",
    "    verbosity=0)\n",
    "latent_results = lpcmci.run_lpcmci(tau_max=tau_max, pc_alpha=alpha_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.plot_time_series_graph(graph=latent_results['graph'], save_name=None, \n",
    "            var_names=var_names, \n",
    "                          figsize=(6, 5), \n",
    "                        )\n",
    "tp.plot_graph(graph=latent_results['graph'], save_name=None, \n",
    "            var_names=var_names, \n",
    "                          figsize=(6, 5), \n",
    "                        ); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LiNGAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to estimate the causal direction for the 0--1 link using a restricted structural causal model, here a LiNGAM assuming non-Gaussianity (of at least one variable) and linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lingam(data, i, j):\n",
    "    \"\"\"Performs bivariate LiNGAM causality test.\n",
    "\n",
    "    The bivariate LiNGAM model assumes linear dependencies and that\n",
    "    either X or eta^Y is non-Gaussian. Here we also assume\n",
    "    no common drivers and that i and j are dependent which needs to\n",
    "    be tested with a correlation test beforehand. The independence\n",
    "    test is done with distance correlation (tigramite package).   \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def indep_test(one, two):\n",
    "        ind_test = GPDC()\n",
    "\n",
    "        array = np.vstack((one, two))\n",
    "        xyz = np.array([0,1])\n",
    "\n",
    "        dim, n = array.shape\n",
    "        value = ind_test.get_dependence_measure(array, xyz)\n",
    "        pval = ind_test.get_analytic_significance(value, T=n, dim=dim, xyz=xyz)\n",
    "\n",
    "        return pval\n",
    "\n",
    "    x = data[:, i].reshape(-1, 1)\n",
    "    y = data[:, j].reshape(-1, 1)\n",
    "\n",
    "    # Test causal model x --> y\n",
    "    beta_hat_y = np.linalg.lstsq(x, y, rcond=None)[0]\n",
    "    yresid = y - np.dot(x, beta_hat_y)\n",
    "    pval_xy = indep_test(x.flatten(), yresid.flatten())\n",
    "\n",
    "    # Test causal model y --> x\n",
    "    beta_hat_x = np.linalg.lstsq(y, x, rcond=None)[0]\n",
    "    xresid = x - np.dot(y, beta_hat_x)\n",
    "    pval_yx = indep_test(y.flatten(), xresid.flatten())\n",
    "\n",
    "    if pval_xy >= pval_yx:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "    return pval, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_effects = results['graph'].copy()\n",
    "\n",
    "# LiNGAM on contemporaneous pairs\n",
    "for i in range(data.shape[1]):\n",
    "    for j in range(i+1, data.shape[1]):\n",
    "        # LiNGAM\n",
    "        if results['graph'][i, j, 0] == 'o-o' or results['graph'][i, j, 0] == 'x-x':\n",
    "            lingam_result = lingam(data[:1000], i, j)\n",
    "            if lingam_result == 1:\n",
    "                print(\"LiNGAM on contemp pair %s --> %s\" % (var_names[i], var_names[j]))\n",
    "                graph_effects[i,j,0] = '-->'\n",
    "                graph_effects[j,i,0] = '<--'\n",
    "            else:\n",
    "                print(\"LiNGAM on contemp pair %s --> %s\" % (var_names[j], var_names[i]))\n",
    "                graph_effects[i,j,0] = '<--'\n",
    "                graph_effects[j,i,0] = '-->'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL GRAPH of causal discovery exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_effects = results['graph'].copy()\n",
    "# Orient link 1 --> 0 based on LiNGAM result.\n",
    "\n",
    "tp.plot_time_series_graph(graph=graph_effects,\n",
    "                        var_names=var_names, \n",
    "                          figsize=(6, 5), \n",
    "#                           special_nodes={(2, 0):'red', (3, 0):\"blue\"}\n",
    "                        )\n",
    "tp.plot_graph(graph=graph_effects, save_name=None, \n",
    "            var_names=var_names, \n",
    "                          figsize=(6, 5), \n",
    "                        ); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating causal effects\n",
    "\n",
    "First, we need a function to give us the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_effect(X, Y, Z=None):\n",
    "    \"\"\"Yields the regression coefficient beta_x in the model\n",
    "    \n",
    "    Y = beta_x * X + beta_z * Z + noise\n",
    "    \n",
    "    Also an intercept is fitted.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_obs = len(Y)\n",
    "    \n",
    "    if Z is None:\n",
    "        return np.linalg.lstsq(np.hstack((X.reshape(len(X), 1), \n",
    "                                          np.ones(n_obs).reshape(-1,1))), \n",
    "                                Y.reshape(-1, 1), \n",
    "                                rcond=None)[0][0]\n",
    "    else:\n",
    "        if Z.ndim == 1:\n",
    "            Z = Z.reshape(len(Z), 1)\n",
    "        return np.linalg.lstsq(np.hstack((X.reshape(len(X), 1), \n",
    "                                          Z, \n",
    "                                          np.ones(n_obs).reshape(-1,1))), \n",
    "                               Y.reshape(-1, 1), \n",
    "                               rcond=None)[0][0]\n",
    "\n",
    "def causal_effect_incl_error(X, Y, Z=None, n_boots=1000):\n",
    "    \"\"\"Bootstrap standard error estimation.\"\"\"\n",
    "    \n",
    "    n_obs = len(Y)\n",
    "\n",
    "    estimate_boot = np.zeros(n_boots)\n",
    "    for b in range(n_boots):\n",
    "        rand = np.random.randint(0, n_obs, n_obs)\n",
    "        if Z is None:\n",
    "            estimate_boot[b] = causal_effect(X[rand], Y[rand], Z=None)      \n",
    "        else:\n",
    "            estimate_boot[b] = causal_effect(X[rand], Y[rand], Z=Z[rand])\n",
    "        \n",
    "    return causal_effect(X, Y, Z=Z), estimate_boot.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at different causal effects among the (lagged) variables. We will use linear regression and can use the learned causal identification methods to identify which conditions (covariates) to use by reading them off the time series graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.plot_time_series_graph(graph=graph_effects,\n",
    "                        var_names=var_names, \n",
    "                          figsize=(6, 5), \n",
    "#                           special_nodes={(2, 0):'red', (3, 0):\"blue\"}\n",
    "                        ); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Causal effect = %.2f +/- %.2f\" % causal_effect_incl_error(X=data[1:,2], \n",
    "                                                                 Y=data[1:,3], \n",
    "                                                                 Z=data[:-1,[1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Causal effect = %.2f +/- %.2f\" % causal_effect_incl_error(X=data[1:,2], \n",
    "                                                                 Y=data[1:,3], \n",
    "                                                                 Z=data[:-1,[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced task: Nonlinear causal effect estimation\n",
    "\n",
    "Below code for you to play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_causal_effect(X, Y, adjustment_set=None, model=None):\n",
    "    causal_effects = CausalEffects(graph=graph_effects, \n",
    "                               graph_type='stationary_dag', \n",
    "                               X=X, Y=Y,  \n",
    "                               verbosity=0)\n",
    "    \n",
    "    if model is None: model = LinearRegression()\n",
    "    # Fit causal effect model from observational data\n",
    "    causal_effects.fit_total_effect(\n",
    "            dataframe=dataframe, \n",
    "            estimator=model,\n",
    "            adjustment_set=adjustment_set,\n",
    "            )\n",
    "\n",
    "    # Set X to intervened values\n",
    "    intervention_data = np.ones((1, 1))\n",
    "    y1 = causal_effects.predict_total_effect( \n",
    "            intervention_data=intervention_data, \n",
    "            )\n",
    "    intervention_data = np.zeros((1, 1))\n",
    "    y2 = causal_effects.predict_total_effect( \n",
    "            intervention_data=intervention_data, \n",
    "            )\n",
    "\n",
    "    for y in Y:\n",
    "        beta = (y1 - y2)\n",
    "        print(\"Causal effect = %.2f\" %(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_causal_effect(X = [(1, -1)], Y = [(3,0)], adjustment_set=[])\n",
    "estimate_causal_effect(X = [(1, -1)], Y = [(3,0)], adjustment_set=[(3, -1), (2, -1)])\n",
    "\n",
    "estimate_causal_effect(X = [(2, 0)], Y = [(3,0)], adjustment_set=[(1, -1), (2, -1)])\n",
    "estimate_causal_effect(X = [(2, 0)], Y = [(3,0)], adjustment_set=[(3, -1),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data(n_obs):\n",
    "    # 0, 1 are uniform, while 2, 3 are Gaussian\n",
    "    X = np.random.randn(n_obs, 4)\n",
    "    X[:, 0] = 2.*np.random.rand(n_obs) - 1\n",
    "    X[:, 1] = 2.*np.random.rand(n_obs) - 1\n",
    "    \n",
    "    for t in range(2, n_obs):\n",
    "        # X[t,1] += 0.9*X[t-1,1\n",
    "        X[t, 0] += .5 * X[t, 1]   #+ 0. * X[t - 1, 0]\n",
    "\n",
    "        X[t, 2] += .75 * X[t - 1, 2] + 0.8 * X[t - 1, 1]\n",
    "        X[t, 3] += .75 * X[t - 1, 3] + 0.8 * X[t, 2]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
